{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Naive RAG System\n",
        "# This notebook implements a basic RAG system using the pre-built database from data_setup.ipynb\n",
        "\n",
        "# Load all required Libraries\n",
        "import pandas as pd\n",
        "import transformers, torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
        "import gc\n",
        "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Pre-built Database and Data\n",
        "\n",
        "**Prerequisites**: Run `data_setup.ipynb` first to create the database and embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-built database and data...\n",
            "Loaded 918 queries\n",
            "Loaded query embeddings: (918, 384)\n",
            "Embedding model loaded\n",
            "✅ Database file found at ../data/processed/rag_wikipedia_mini.db\n",
            "Connected to existing Milvus database\n"
          ]
        }
      ],
      "source": [
        "# Load pre-built data and database\n",
        "print(\"Loading pre-built database and data...\")\n",
        "\n",
        "# Load queries from saved CSV\n",
        "queries = pd.read_csv(\"../data/processed/queries.csv\")\n",
        "print(f\"Loaded {len(queries)} queries\")\n",
        "\n",
        "# Load pre-computed query embeddings\n",
        "query_embeddings = np.load(\"../data/processed/query_embeddings.npy\")\n",
        "print(f\"Loaded query embeddings: {query_embeddings.shape}\")\n",
        "\n",
        "# Initialize embedding model (for consistency)\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded\")\n",
        "\n",
        "# Check if database file exists\n",
        "import os\n",
        "db_path = \"../data/processed/rag_wikipedia_mini.db\"\n",
        "\n",
        "print(f\"✅ Database file found at {db_path}\")\n",
        "\n",
        "client = MilvusClient(db_path)\n",
        "print(\"Connected to existing Milvus database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search function defined for vector database queries\n"
          ]
        }
      ],
      "source": [
        "# Define search function for vector database\n",
        "def search_and_fetch_top_n_passages(query_emb, limit=3):\n",
        "    \"\"\"\n",
        "    Search for similar passages in the vector database\n",
        "    \n",
        "    Args:\n",
        "        query_emb: Query embedding vector\n",
        "        limit: Number of top results to return\n",
        "    \n",
        "    Returns:\n",
        "        Search results from Milvus\n",
        "    \"\"\"\n",
        "    search_params = {\n",
        "        \"metric_type\": \"COSINE\",\n",
        "        \"params\": {\"nprobe\": 10}\n",
        "    }\n",
        "    \n",
        "    output_ = client.search(\n",
        "        collection_name=\"rag_mini\",\n",
        "        data=[query_emb.tolist()],\n",
        "        anns_field=\"embedding\",\n",
        "        search_params=search_params,\n",
        "        limit=limit,\n",
        "        output_fields=[\"passage\"]\n",
        "    )\n",
        "    return output_\n",
        "\n",
        "print(\"Search function defined for vector database queries\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Response for a Single Query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model: google/flan-t5-base\n",
            "Model parameters: 247,577,856\n"
          ]
        }
      ],
      "source": [
        "# Load the LLM Model you want to use\n",
        "# Using a smaller model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "    \n",
        "    print(f\"Loaded model: {model_name}\")\n",
        "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test query: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "\n",
            "Context (top 3 passages):\n",
            "\n",
            "Passage 1: Young Abraham Lincoln...\n",
            "\n",
            "Passage 2: Abraham Lincoln (February 12, 1809 â April 15, 1865) was the sixteenth President of the United Sta...\n",
            "\n",
            "Passage 3: Sixteen months before his death, his son, John Quincy Adams, became the sixth President of the Unite...\n",
            "\n",
            "Prompt: You are a helpful assistant that answers questions based on the provided context. \n",
            "Use only the information from the context to answer the question. \n",
            "If the context doesn't contain enough information ...\n"
          ]
        }
      ],
      "source": [
        "# Test with a single query\n",
        "query = queries['question'].iloc[0]  # First query\n",
        "query_embedding = query_embeddings[0]\n",
        "\n",
        "print(f\"Test query: {query}\")\n",
        "\n",
        "# Search for similar passages\n",
        "search_results = search_and_fetch_top_n_passages(query_embedding, 3)\n",
        "\n",
        "# Extract context from search results\n",
        "top_3_passages = []\n",
        "for i in range(min(3, len(search_results[0]))):\n",
        "    top_3_passages.append(search_results[0][i]['entity']['passage'])\n",
        "\n",
        "context = \"\\n\\n\".join(top_3_passages)\n",
        "print(f\"\\nContext (top 3 passages):\")\n",
        "for i, passage in enumerate(top_3_passages):\n",
        "    print(f\"\\nPassage {i+1}: {passage[:100]}...\")\n",
        "\n",
        "# Create prompt\n",
        "system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
        "Use only the information from the context to answer the question. \n",
        "If the context doesn't contain enough information to answer the question, say \"not enough information\".\n",
        "Be concise and accurate in your response.\"\"\"\n",
        "\n",
        "prompt = f\"\"\"{system_prompt}\\n\n",
        "Context: {context}\\n\n",
        "Question: {query}\"\"\"\n",
        "\n",
        "print(f\"\\nPrompt: {prompt[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "Generated Answer: yes.\n"
          ]
        }
      ],
      "source": [
        "# Generate answer with proper memory management\n",
        "try:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_length=150,\n",
        "            num_beams=4,\n",
        "            early_stopping=True,\n",
        "            do_sample=False\n",
        "        )\n",
        "    \n",
        "    # Decode and extract answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Generated Answer: {answer}\")\n",
        "    \n",
        "    # Clear memory after generation\n",
        "    del inputs, outputs\n",
        "    gc.collect()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Answer generation failed: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multiprocessing RAG Functions with Resume Capability\n",
        "def process_single_query(args):\n",
        "    \"\"\"\n",
        "    Process a single query through the RAG pipeline\n",
        "    \n",
        "    Args:\n",
        "        args: tuple containing (query_idx, question, embedding, client, tokenizer, model, system_prompt, n, existing_answers)\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (query_idx, generated_answer, combined_context, success, skipped)\n",
        "    \"\"\"\n",
        "    query_idx, question, embedding, client, tokenizer, model, system_prompt, n, existing_answers = args\n",
        "    \n",
        "    # Check if query already has an answer\n",
        "    if query_idx < len(existing_answers) and existing_answers[query_idx] and existing_answers[query_idx] != \"\":\n",
        "        return (query_idx, existing_answers[query_idx], \"\", True, True)  # skipped=True\n",
        "    \n",
        "    try:\n",
        "        # Search for similar passages\n",
        "        search_results = search_and_fetch_top_n_passages(embedding, n)\n",
        "        \n",
        "        # Extract top n passages as context\n",
        "        top_n_passages = []\n",
        "        for i in range(min(n, len(search_results[0]))):\n",
        "            top_n_passages.append(search_results[0][i]['entity']['passage'])\n",
        "        \n",
        "        # Combine all contexts into a single string\n",
        "        combined_context = \"\\n\\n\".join(top_n_passages)\n",
        "        \n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"{system_prompt}\\n\n",
        "        Context: {combined_context}\\n\n",
        "        Question: {question}\"\"\"\n",
        "        \n",
        "        # Generate answer\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_length=150,\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                do_sample=False\n",
        "            )\n",
        "        \n",
        "        # Decode the generated answer\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Clear memory\n",
        "        del inputs, outputs\n",
        "        gc.collect()\n",
        "        \n",
        "        return (query_idx, answer, combined_context, True, False)  # skipped=False\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error processing query {query_idx + 1}: {e}\")\n",
        "        return (query_idx, \"Error generating answer\", \"\", False, False)\n",
        "\n",
        "def process_queries_parallel(queries, query_embeddings, client, tokenizer, model, system_prompt, n=3, max_workers=4, save_interval=50):\n",
        "    \"\"\"\n",
        "    Process all queries in parallel using ThreadPoolExecutor with resume capability\n",
        "    \n",
        "    Args:\n",
        "        queries: DataFrame with questions\n",
        "        query_embeddings: numpy array of query embeddings\n",
        "        client: Milvus client\n",
        "        tokenizer: Model tokenizer\n",
        "        model: Language model\n",
        "        system_prompt: System prompt string\n",
        "        n: Number of top contexts to retrieve\n",
        "        max_workers: Number of parallel workers\n",
        "        save_interval: Save progress every N queries\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (generated_answers, combined_contexts, success_count, skipped_count)\n",
        "    \"\"\"\n",
        "    print(f\"Starting parallel processing with {max_workers} workers...\")\n",
        "    print(f\"Processing {len(queries)} queries...\")\n",
        "    \n",
        "    # Check for existing results\n",
        "    existing_answers = []\n",
        "    existing_contexts = []\n",
        "    \n",
        "    if 'generated_answer' in queries.columns:\n",
        "        existing_answers = queries['generated_answer'].fillna(\"\").tolist()\n",
        "        print(f\"Found {sum(1 for ans in existing_answers if ans and ans != '')} existing answers\")\n",
        "    \n",
        "    if 'combined_context' in queries.columns:\n",
        "        existing_contexts = queries['combined_context'].fillna(\"\").tolist()\n",
        "    \n",
        "    # Prepare arguments for each query\n",
        "    query_args = []\n",
        "    for idx, (question, embedding) in enumerate(zip(queries['question'], query_embeddings)):\n",
        "        query_args.append((idx, question, embedding, client, tokenizer, model, system_prompt, n, existing_answers))\n",
        "    \n",
        "    # Initialize results storage\n",
        "    generated_answers = existing_answers[:] if existing_answers else [\"\"] * len(queries)\n",
        "    combined_contexts = existing_contexts[:] if existing_contexts else [\"\"] * len(queries)\n",
        "    success_count = 0\n",
        "    skipped_count = 0\n",
        "    \n",
        "    # Process queries in parallel\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # Submit all tasks\n",
        "        future_to_idx = {executor.submit(process_single_query, args): args[0] for args in query_args}\n",
        "        \n",
        "        # Process completed tasks with progress bar\n",
        "        with tqdm(total=len(queries), desc=\"Processing queries\") as pbar:\n",
        "            for i, future in enumerate(as_completed(future_to_idx)):\n",
        "                query_idx, answer, context, success, skipped = future.result()\n",
        "                \n",
        "                # Store results\n",
        "                generated_answers[query_idx] = answer\n",
        "                if context:  # Only update context if we have a new one\n",
        "                    combined_contexts[query_idx] = context\n",
        "                \n",
        "                if success:\n",
        "                    success_count += 1\n",
        "                if skipped:\n",
        "                    skipped_count += 1\n",
        "                \n",
        "                pbar.update(1)\n",
        "                \n",
        "                # Periodic saving to prevent data loss\n",
        "                if (i + 1) % save_interval == 0:\n",
        "                    temp_df = queries.copy()\n",
        "                    temp_df['generated_answer'] = generated_answers\n",
        "                    temp_df['combined_context'] = combined_contexts\n",
        "                    temp_df.to_csv(\"../results/rag_generated_answers_temp.csv\", index=False)\n",
        "                    print(f\"\\n💾 Progress saved at query {i + 1}\")\n",
        "    \n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    \n",
        "    print(f\"\\nParallel processing completed!\")\n",
        "    print(f\"Total time: {processing_time:.2f} seconds\")\n",
        "    print(f\"Average time per query: {processing_time/len(queries):.2f} seconds\")\n",
        "    print(f\"Successful queries: {success_count}/{len(queries)}\")\n",
        "    print(f\"Skipped queries: {skipped_count}/{len(queries)}\")\n",
        "    print(f\"Speed improvement: ~{max_workers}x faster than sequential processing\")\n",
        "    \n",
        "    return generated_answers, combined_contexts, success_count, skipped_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Responses for all the Queries in the Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1759531997.669011 2883788 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting parallel processing with 4 workers...\n",
            "Processing 918 queries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:   0%|          | 0/918 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing queries:   5%|▌         | 50/918 [01:56<08:37,  1.68it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  11%|█         | 100/918 [03:24<17:42,  1.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  16%|█▋        | 150/918 [05:12<20:33,  1.61s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  22%|██▏       | 200/918 [07:06<24:02,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  27%|██▋       | 250/918 [08:26<15:51,  1.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  33%|███▎      | 300/918 [09:49<14:53,  1.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  38%|███▊      | 350/918 [11:05<16:28,  1.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 350\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  44%|████▎     | 400/918 [13:01<08:37,  1.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  49%|████▉     | 450/918 [14:27<05:58,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 450\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  54%|█████▍    | 500/918 [16:05<10:01,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  60%|█████▉    | 550/918 [17:21<05:00,  1.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  65%|██████▌   | 600/918 [18:18<03:04,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  71%|███████   | 650/918 [20:30<10:34,  2.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  76%|███████▋  | 700/918 [22:38<17:33,  4.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  82%|████████▏ | 750/918 [25:06<06:46,  2.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  87%|████████▋ | 801/918 [27:33<07:03,  3.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  93%|█████████▎| 850/918 [30:26<05:49,  5.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries:  98%|█████████▊| 900/918 [32:38<01:29,  4.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💾 Progress saved at query 900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing queries: 100%|██████████| 918/918 [33:11<00:00,  2.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Parallel processing completed!\n",
            "Total time: 1991.39 seconds\n",
            "Average time per query: 2.17 seconds\n",
            "Successful queries: 918/918\n",
            "Skipped queries: 0/918\n",
            "Speed improvement: ~4x faster than sequential processing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run process_queries_parallel\n",
        "\n",
        "generated_answers, combined_contexts, success_count, skipped_count = process_queries_parallel(\n",
        "    queries,\n",
        "    query_embeddings,\n",
        "    client,\n",
        "    tokenizer,\n",
        "    model,\n",
        "    system_prompt,\n",
        "    n=3,\n",
        "    max_workers=4,\n",
        "    save_interval=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Final results saved to ../results/naive_rag_answers.csv\n"
          ]
        }
      ],
      "source": [
        "# Save final results\n",
        "queries['generated_answer'] = generated_answers\n",
        "queries['combined_context'] = combined_contexts\n",
        "queries.to_csv(\"../results/naive_rag_answers.csv\", index=False)\n",
        "print(f\"\\n✅ Final results saved to ../results/naive_rag_answers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🗑️ Removed temporary file: ../results/rag_generated_answers_temp.csv\n"
          ]
        }
      ],
      "source": [
        "# Remove temporary file if exists\n",
        "temp_file_path = \"../results/rag_generated_answers_temp.csv\"\n",
        "if os.path.exists(temp_file_path):\n",
        "    os.remove(temp_file_path)\n",
        "    print(f\"🗑️ Removed temporary file: {temp_file_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
