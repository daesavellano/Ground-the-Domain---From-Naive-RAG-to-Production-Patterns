{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42eba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG System with Reranking and Query Rewriting\n",
    "import pandas as pd\n",
    "import transformers, torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import re\n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538a0e4",
   "metadata": {},
   "source": [
    "# Enhanced RAG System\n",
    "\n",
    "This notebook implements an enhanced RAG system with two key improvements:\n",
    "\n",
    "1. **Query Rewriting**: Expands and improves queries for better retrieval\n",
    "2. **Reranking**: Uses cross-encoder models to rerank retrieved passages for better relevance\n",
    "\n",
    "## Key Features:\n",
    "- Query expansion and rewriting\n",
    "- Cross-encoder reranking for better passage selection\n",
    "- Multiprocessing for faster processing\n",
    "- Resume capability for interrupted runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd866f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-built database and data for Enhanced RAG...\n",
      "Loaded 918 queries\n",
      "Loaded query embeddings: (918, 384)\n",
      "Embedding model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to existing Milvus database\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-built Database and Data\n",
    "\n",
    "# **Prerequisites**: Run `data_setup.ipynb` first to create the database and embeddings.\n",
    "\n",
    "# Load pre-built data and database\n",
    "print(\"Loading pre-built database and data for Enhanced RAG...\")\n",
    "\n",
    "# Load queries from saved CSV\n",
    "queries = pd.read_csv(\"../data/processed/queries.csv\")\n",
    "print(f\"Loaded {len(queries)} queries\")\n",
    "\n",
    "# Load pre-computed query embeddings\n",
    "query_embeddings = np.load(\"../data/processed/query_embeddings.npy\")\n",
    "print(f\"Loaded query embeddings: {query_embeddings.shape}\")\n",
    "\n",
    "# Initialize embedding model (for consistency)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Embedding model loaded\")\n",
    "\n",
    "# Connect to existing Milvus database\n",
    "client = MilvusClient(\"../data/processed/rag_wikipedia_mini.db\")\n",
    "print(\"Connected to existing Milvus database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea35f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced RAG components loaded:\n",
      "- Query rewriter: google/flan-t5-base\n",
      "- Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "- Generation model: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "# Initialize Enhanced RAG Components\n",
    "\n",
    "# 1. Query Rewriting Model (T5-based for query expansion)\n",
    "query_rewriter_model = \"google/flan-t5-base\"\n",
    "query_rewriter_tokenizer = AutoTokenizer.from_pretrained(query_rewriter_model)\n",
    "query_rewriter = AutoModelForSeq2SeqLM.from_pretrained(query_rewriter_model, dtype=torch.float32)\n",
    "\n",
    "# 2. Reranking Model (Cross-encoder for passage reranking)\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "# 3. Generation Model (for final answer generation)\n",
    "generation_model_name = \"google/flan-t5-base\"\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name)\n",
    "generation_model = AutoModelForSeq2SeqLM.from_pretrained(generation_model_name, dtype=torch.float32)\n",
    "\n",
    "print(\"Enhanced RAG components loaded:\")\n",
    "print(f\"- Query rewriter: {query_rewriter_model}\")\n",
    "print(f\"- Reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(f\"- Generation model: {generation_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff99e633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced RAG functions defined:\n",
      "- rewrite_query(): Expands queries for better retrieval\n",
      "- rerank_passages(): Reranks passages using cross-encoder\n",
      "- search_and_fetch_top_n_passages(): Vector search function\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG Functions\n",
    "\n",
    "def rewrite_query(original_query: str, tokenizer, model) -> str:\n",
    "    \"\"\"\n",
    "    Rewrite and expand the original query for better retrieval\n",
    "    \n",
    "    Args:\n",
    "        original_query: The original user query\n",
    "        tokenizer: Query rewriter tokenizer\n",
    "        model: Query rewriter model\n",
    "    \n",
    "    Returns:\n",
    "        str: Rewritten/expanded query\n",
    "    \"\"\"\n",
    "    # Create a prompt for query expansion\n",
    "    expansion_prompt = f\"\"\"Rewrite and expand this question to make it more specific and searchable. \n",
    "    Include relevant keywords and context that would help find better information.\n",
    "    \n",
    "    Original question: {original_query}\n",
    "    \n",
    "    Expanded question:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer(expansion_prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=100,\n",
    "                num_beams=3,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        expanded_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up the output\n",
    "        expanded_query = expanded_query.strip()\n",
    "        \n",
    "        # If expansion failed or is too short, return original\n",
    "        if len(expanded_query) < len(original_query) * 0.5:\n",
    "            return original_query\n",
    "            \n",
    "        return expanded_query\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Query rewriting failed: {e}\")\n",
    "        return original_query\n",
    "\n",
    "def rerank_passages(query: str, passages: List[str], reranker_model, top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Rerank retrieved passages using a cross-encoder model\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        passages: List of retrieved passages\n",
    "        reranker_model: Cross-encoder reranking model\n",
    "        top_k: Number of top passages to return\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: (passage, relevance_score)\n",
    "    \"\"\"\n",
    "    if not passages:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Create query-passage pairs for reranking\n",
    "        pairs = [(query, passage) for passage in passages]\n",
    "        \n",
    "        # Get relevance scores from cross-encoder\n",
    "        scores = reranker_model.predict(pairs)\n",
    "        \n",
    "        # Combine passages with scores and sort by relevance\n",
    "        passage_scores = list(zip(passages, scores))\n",
    "        passage_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k passages\n",
    "        return passage_scores[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Reranking failed: {e}\")\n",
    "        # Return original passages if reranking fails\n",
    "        return [(passage, 0.0) for passage in passages[:top_k]]\n",
    "\n",
    "def search_and_fetch_top_n_passages(query_emb, limit=10):\n",
    "    \"\"\"\n",
    "    Search for similar passages in the vector database (same as naive RAG)\n",
    "    \"\"\"\n",
    "    search_params = {\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {\"nprobe\": 10}\n",
    "    }\n",
    "    \n",
    "    output_ = client.search(\n",
    "        collection_name=\"rag_mini\",\n",
    "        data=[query_emb.tolist()],\n",
    "        anns_field=\"embedding\",\n",
    "        search_params=search_params,\n",
    "        limit=limit,\n",
    "        output_fields=[\"passage\"]\n",
    "    )\n",
    "    return output_\n",
    "\n",
    "print(\"Enhanced RAG functions defined:\")\n",
    "print(\"- rewrite_query(): Expands queries for better retrieval\")\n",
    "print(\"- rerank_passages(): Reranks passages using cross-encoder\")\n",
    "print(\"- search_and_fetch_top_n_passages(): Vector search function\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a01923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced RAG pipeline functions defined:\n",
      "- process_single_enhanced_query(): Single query processing with rewriting + reranking\n",
      "- process_enhanced_queries_parallel(): Parallel processing with all enhancements\n"
     ]
    }
   ],
   "source": [
    "# Enhanced RAG Pipeline with Multiprocessing\n",
    "\n",
    "def process_single_enhanced_query(args):\n",
    "    \"\"\"\n",
    "    Process a single query through the enhanced RAG pipeline\n",
    "    \n",
    "    Args:\n",
    "        args: tuple containing (query_idx, question, embedding, client, embedding_model, \n",
    "              query_rewriter_tokenizer, query_rewriter, reranker, generation_tokenizer, \n",
    "              generation_model, system_prompt, n)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (query_idx, generated_answer, combined_context, success, skipped)\n",
    "    \"\"\"\n",
    "    (query_idx, question, embedding, client, embedding_model, \n",
    "     query_rewriter_tokenizer, query_rewriter, reranker, \n",
    "     generation_tokenizer, generation_model, system_prompt, n, existing_answers) = args\n",
    "    \n",
    "    # Check if query already has an answer\n",
    "    if query_idx < len(existing_answers) and existing_answers[query_idx] and existing_answers[query_idx] != \"\":\n",
    "        return (query_idx, existing_answers[query_idx], \"\", True, True)  # skipped=True\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Query Rewriting\n",
    "        rewritten_query = rewrite_query(question, query_rewriter_tokenizer, query_rewriter)\n",
    "        \n",
    "        # Step 2: Generate embedding for rewritten query\n",
    "        rewritten_embedding = embedding_model.encode([rewritten_query])[0]\n",
    "        \n",
    "        # Step 3: Retrieve more passages (we'll rerank them)\n",
    "        search_results = search_and_fetch_top_n_passages(rewritten_embedding, limit=10)\n",
    "        \n",
    "        # Extract passages\n",
    "        retrieved_passages = []\n",
    "        for i in range(len(search_results[0])):\n",
    "            retrieved_passages.append(search_results[0][i]['entity']['passage'])\n",
    "        \n",
    "        # Step 4: Rerank passages using cross-encoder\n",
    "        reranked_passages = rerank_passages(rewritten_query, retrieved_passages, reranker, top_k=n)\n",
    "        \n",
    "        # Extract top reranked passages\n",
    "        top_passages = [passage for passage, score in reranked_passages]\n",
    "        combined_context = \"\\n\\n\".join(top_passages)\n",
    "        \n",
    "        # Step 5: Generate answer with enhanced context\n",
    "        prompt = f\"\"\"{system_prompt}\\n\n",
    "        Context: {combined_context}\\n\n",
    "        Question: {question}\"\"\"\n",
    "        \n",
    "        inputs = generation_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = generation_model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=150,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        # Decode the generated answer\n",
    "        answer = generation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Clear memory\n",
    "        del inputs, outputs\n",
    "        gc.collect()\n",
    "        \n",
    "        return (query_idx, answer, combined_context, True, False)  # skipped=False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing enhanced query {query_idx + 1}: {e}\")\n",
    "        return (query_idx, \"Error generating answer\", \"\", False, False)\n",
    "\n",
    "def process_enhanced_queries_parallel(queries, query_embeddings, client, embedding_model, \n",
    "                                    query_rewriter_tokenizer, query_rewriter, reranker,\n",
    "                                    generation_tokenizer, generation_model, system_prompt, \n",
    "                                    n=3, max_workers=4, save_interval=50):\n",
    "    \"\"\"\n",
    "    Process all queries using enhanced RAG pipeline with multiprocessing\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (generated_answers, combined_contexts, success_count, skipped_count)\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting Enhanced RAG processing with {max_workers} workers...\")\n",
    "    print(f\"Processing {len(queries)} queries with query rewriting and reranking...\")\n",
    "    \n",
    "    # Check for existing results\n",
    "    existing_answers = []\n",
    "    existing_contexts = []\n",
    "    \n",
    "    if 'generated_answer' in queries.columns:\n",
    "        existing_answers = queries['generated_answer'].fillna(\"\").tolist()\n",
    "        print(f\"Found {sum(1 for ans in existing_answers if ans and ans != '')} existing answers\")\n",
    "    \n",
    "    if 'combined_context' in queries.columns:\n",
    "        existing_contexts = queries['combined_context'].fillna(\"\").tolist()\n",
    "    \n",
    "    # Prepare arguments for each query\n",
    "    query_args = []\n",
    "    for idx, (question, embedding) in enumerate(zip(queries['question'], query_embeddings)):\n",
    "        query_args.append((idx, question, embedding, client, embedding_model,\n",
    "                          query_rewriter_tokenizer, query_rewriter, reranker,\n",
    "                          generation_tokenizer, generation_model, system_prompt, n, existing_answers))\n",
    "    \n",
    "    # Initialize results storage\n",
    "    generated_answers = existing_answers[:] if existing_answers else [\"\"] * len(queries)\n",
    "    combined_contexts = existing_contexts[:] if existing_contexts else [\"\"] * len(queries)\n",
    "    success_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process queries in parallel\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_idx = {executor.submit(process_single_enhanced_query, args): args[0] for args in query_args}\n",
    "        \n",
    "        # Process completed tasks with progress bar\n",
    "        with tqdm(total=len(queries), desc=\"Enhanced RAG Processing\") as pbar:\n",
    "            for i, future in enumerate(as_completed(future_to_idx)):\n",
    "                query_idx, answer, context, success, skipped = future.result()\n",
    "                \n",
    "                # Store results\n",
    "                generated_answers[query_idx] = answer\n",
    "                if context:  # Only update context if we have a new one\n",
    "                    combined_contexts[query_idx] = context\n",
    "                \n",
    "                if success:\n",
    "                    success_count += 1\n",
    "                if skipped:\n",
    "                    skipped_count += 1\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Periodic saving to prevent data loss\n",
    "                if (i + 1) % save_interval == 0:\n",
    "                    temp_df = queries.copy()\n",
    "                    temp_df['generated_answer'] = generated_answers\n",
    "                    temp_df['combined_context'] = combined_contexts\n",
    "                    temp_df.to_csv(\"../results/enhanced_rag_generated_answers_temp.csv\", index=False)\n",
    "                    print(f\"\\n💾 Enhanced RAG progress saved at query {i + 1}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    processing_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n🎉 Enhanced RAG processing completed!\")\n",
    "    print(f\"Total time: {processing_time:.2f} seconds\")\n",
    "    print(f\"Average time per query: {processing_time/len(queries):.2f} seconds\")\n",
    "    print(f\"Successful queries: {success_count}/{len(queries)}\")\n",
    "    print(f\"Skipped queries: {skipped_count}/{len(queries)}\")\n",
    "    print(f\"Features used: Query Rewriting + Reranking + Multiprocessing\")\n",
    "    \n",
    "    return generated_answers, combined_contexts, success_count, skipped_count\n",
    "\n",
    "print(\"Enhanced RAG pipeline functions defined:\")\n",
    "print(\"- process_single_enhanced_query(): Single query processing with rewriting + reranking\")\n",
    "print(\"- process_enhanced_queries_parallel(): Parallel processing with all enhancements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "219dbfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Enhanced RAG on a single query...\n",
      "Original query: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Rewritten query: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "\n",
      "Retrieved 5 passages\n",
      "\n",
      "Reranked passages:\n",
      "Passage 1 (score: 9.051): Abraham Lincoln (February 12, 1809 â April 15, 1865) was the sixteenth President of the United Sta...\n",
      "Passage 2 (score: 7.637): On November 6, 1860, Lincoln was elected as the 16th President of the United States, beating Democra...\n",
      "Passage 3 (score: -1.837): Young Abraham Lincoln...\n",
      "\n",
      "🎯 Enhanced RAG Result:\n",
      "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Answer: yes.\n",
      "Context length: 2559 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Enhanced RAG on a Single Query\n",
    "print(\"🧪 Testing Enhanced RAG on a single query...\")\n",
    "\n",
    "# Test query\n",
    "test_query = queries['question'].iloc[0]\n",
    "test_embedding = embedding_model.encode([test_query])[0]\n",
    "\n",
    "print(f\"Original query: {test_query}\")\n",
    "\n",
    "# Step 1: Query Rewriting\n",
    "rewritten_query = rewrite_query(test_query, query_rewriter_tokenizer, query_rewriter)\n",
    "print(f\"Rewritten query: {rewritten_query}\")\n",
    "\n",
    "# Step 2: Search with rewritten query\n",
    "rewritten_embedding = embedding_model.encode([rewritten_query])[0]\n",
    "search_results = search_and_fetch_top_n_passages(rewritten_embedding, limit=5)\n",
    "\n",
    "# Extract passages\n",
    "retrieved_passages = []\n",
    "for i in range(len(search_results[0])):\n",
    "    retrieved_passages.append(search_results[0][i]['entity']['passage'])\n",
    "\n",
    "print(f\"\\nRetrieved {len(retrieved_passages)} passages\")\n",
    "\n",
    "# Step 3: Rerank passages\n",
    "reranked_passages = rerank_passages(rewritten_query, retrieved_passages, reranker, top_k=3)\n",
    "\n",
    "print(f\"\\nReranked passages:\")\n",
    "for i, (passage, score) in enumerate(reranked_passages):\n",
    "    print(f\"Passage {i+1} (score: {score:.3f}): {passage[:100]}...\")\n",
    "\n",
    "# Step 4: Generate answer\n",
    "system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "Use only the information from the context to answer the question. \n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "Be concise and accurate in your response.\"\"\"\n",
    "\n",
    "top_passages = [passage for passage, score in reranked_passages]\n",
    "combined_context = \"\\n\\n\".join(top_passages)\n",
    "\n",
    "prompt = f\"\"\"{system_prompt}\\n\n",
    "Context: {combined_context}\\n\n",
    "Question: {test_query}\"\"\"\n",
    "\n",
    "inputs = generation_tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = generation_model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=150,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "answer = generation_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n🎯 Enhanced RAG Result:\")\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"Context length: {len(combined_context)} characters\")\n",
    "\n",
    "# Clear memory\n",
    "del inputs, outputs\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa3d5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Enhanced RAG processing for all queries...\n",
      "No existing temporary file found, starting fresh...\n",
      "Generated embeddings for 918 queries\n",
      "🚀 Starting Enhanced RAG processing with 4 workers...\n",
      "Processing 918 queries with query rewriting and reranking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:   5%|▌         | 50/918 [06:30<51:29,  3.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  11%|█         | 100/918 [12:26<1:21:57,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  16%|█▋        | 150/918 [17:40<48:32,  3.79s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  22%|██▏       | 200/918 [24:10<1:44:29,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  27%|██▋       | 250/918 [27:32<45:48,  4.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  33%|███▎      | 300/918 [32:00<50:54,  4.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  38%|███▊      | 350/918 [37:36<1:25:18,  9.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  44%|████▎     | 401/918 [41:52<30:42,  3.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  49%|████▉     | 450/918 [47:08<1:21:18, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  54%|█████▍    | 500/918 [52:05<26:30,  3.81s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  60%|██████    | 551/918 [57:37<53:56,  8.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  65%|██████▌   | 600/918 [1:03:34<30:08,  5.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  71%|███████   | 651/918 [1:11:15<23:50,  5.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  76%|███████▋  | 700/918 [1:17:57<19:09,  5.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  82%|████████▏ | 750/918 [1:24:22<15:42,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  87%|████████▋ | 800/918 [1:31:57<22:55, 11.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  93%|█████████▎| 850/918 [1:40:06<10:39,  9.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing:  98%|█████████▊| 900/918 [1:49:38<02:05,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 Enhanced RAG progress saved at query 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG Processing: 100%|██████████| 918/918 [1:54:44<00:00,  7.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 Enhanced RAG processing completed!\n",
      "Total time: 6884.45 seconds\n",
      "Average time per query: 7.50 seconds\n",
      "Successful queries: 918/918\n",
      "Skipped queries: 0/918\n",
      "Features used: Query Rewriting + Reranking + Multiprocessing\n",
      "\n",
      "🎉 Enhanced RAG Results:\n",
      "Total queries processed: 918\n",
      "Successful queries: 918\n",
      "Skipped queries: 0\n",
      "Success rate: 100.0%\n",
      "Enhanced features: Query Rewriting + Reranking + Multiprocessing\n"
     ]
    }
   ],
   "source": [
    "# Process All Queries with Enhanced RAG\n",
    "print(\"🚀 Starting Enhanced RAG processing for all queries...\")\n",
    "\n",
    "# Check for existing temporary file (crash recovery)\n",
    "import os\n",
    "temp_file_path = \"../results/enhanced_rag_generated_answers_temp.csv\"\n",
    "if os.path.exists(temp_file_path):\n",
    "    print(f\"🔄 Found existing temporary file: {temp_file_path}\")\n",
    "    print(\"Loading existing progress to resume processing...\")\n",
    "    temp_queries = pd.read_csv(temp_file_path)\n",
    "    \n",
    "    # Check if we have any existing results\n",
    "    if 'generated_answer' in temp_queries.columns:\n",
    "        existing_count = sum(1 for ans in temp_queries['generated_answer'].fillna(\"\") if ans and ans != \"\")\n",
    "        print(f\"Found {existing_count} existing answers in temporary file\")\n",
    "        \n",
    "        # Update queries with existing results\n",
    "        queries = temp_queries.copy()\n",
    "        print(\"Resuming from existing progress...\")\n",
    "    else:\n",
    "        print(\"Temporary file found but no valid results, starting fresh...\")\n",
    "else:\n",
    "    print(\"No existing temporary file found, starting fresh...\")\n",
    "\n",
    "# Generate query embeddings\n",
    "query_embeddings = embedding_model.encode(queries['question'].tolist())\n",
    "print(f\"Generated embeddings for {len(query_embeddings)} queries\")\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"\"\"You are a helpful assistant that answers questions based on the provided context. \n",
    "Use only the information from the context to answer the question. \n",
    "If the context doesn't contain enough information to answer the question, say so.\n",
    "Be concise and accurate in your response.\"\"\"\n",
    "\n",
    "# Enhanced RAG parameters\n",
    "n = 3  # Use top 3 reranked contexts\n",
    "max_workers = 4  # Number of parallel workers\n",
    "save_interval = 50  # Save progress every 50 queries\n",
    "\n",
    "# Process all queries with enhanced RAG\n",
    "generated_answers, combined_contexts, success_count, skipped_count = process_enhanced_queries_parallel(\n",
    "    queries=queries,\n",
    "    query_embeddings=query_embeddings,\n",
    "    client=client,\n",
    "    embedding_model=embedding_model,\n",
    "    query_rewriter_tokenizer=query_rewriter_tokenizer,\n",
    "    query_rewriter=query_rewriter,\n",
    "    reranker=reranker,\n",
    "    generation_tokenizer=generation_tokenizer,\n",
    "    generation_model=generation_model,\n",
    "    system_prompt=system_prompt,\n",
    "    n=n,\n",
    "    max_workers=max_workers,\n",
    "    save_interval=save_interval\n",
    ")\n",
    "\n",
    "# Add results to the queries dataframe\n",
    "queries['generated_answer'] = generated_answers\n",
    "queries['combined_context'] = combined_contexts\n",
    "\n",
    "print(f\"\\n🎉 Enhanced RAG Results:\")\n",
    "print(f\"Total queries processed: {len(queries)}\")\n",
    "print(f\"Successful queries: {success_count}\")\n",
    "print(f\"Skipped queries: {skipped_count}\")\n",
    "print(f\"Success rate: {success_count/len(queries)*100:.1f}%\")\n",
    "print(f\"Enhanced features: Query Rewriting + Reranking + Multiprocessing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1eb4e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saving Enhanced RAG results...\n",
      "Enhanced RAG results saved to: ../results/enhanced_rag_answers.csv\n",
      "\n",
      "📊 Sample Enhanced RAG Results:\n",
      "\n",
      "--- Query 1 ---\n",
      "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Ground Truth: yes\n",
      "Enhanced RAG Answer: yes.\n",
      "Context Length: 2559 characters\n",
      "Context Preview: Abraham Lincoln (February 12, 1809 â April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassin...\n",
      "\n",
      "--- Query 2 ---\n",
      "Question: Did Lincoln sign the National Banking Act of 1863?\n",
      "Ground Truth: yes\n",
      "Enhanced RAG Answer: Yes.\n",
      "Context Length: 2246 characters\n",
      "Context Preview: Lincoln believed in the Whig theory of the presidency, which left Congress to write the laws while he signed them, vetoing only those bills that threa...\n",
      "\n",
      "--- Query 3 ---\n",
      "Question: Did his mother die of pneumonia?\n",
      "Ground Truth: no\n",
      "Enhanced RAG Answer: No.\n",
      "Context Length: 1184 characters\n",
      "Context Preview: Alice Hathaway Lee Roosevelt (July 29, 1861 in Chestnut Hill, Massachusetts â February 14 1884 in Manhattan, New York) was the first wife of Theodor...\n",
      "\n",
      "--- Query 4 ---\n",
      "Question: How many long was Lincoln's formal education?\n",
      "Ground Truth: 18 months\n",
      "Enhanced RAG Answer: 18 months.\n",
      "Context Length: 2550 characters\n",
      "Context Preview: Lincoln's formal education consisted of about 18 months of schooling. Largely self-educated, he read every book he could get his hands on, once walkin...\n",
      "\n",
      "--- Query 5 ---\n",
      "Question: When did Lincoln begin his political career?\n",
      "Ground Truth: 1832\n",
      "Enhanced RAG Answer: 1832.\n",
      "Context Length: 2400 characters\n",
      "Context Preview: Lincoln began his political career in 1832, at age 23, with an unsuccessful campaign for the Illinois General Assembly, as a member of the Whig Party....\n",
      "\n",
      "✅ Enhanced RAG processing completed!\n",
      "Results saved with query rewriting and reranking improvements\n"
     ]
    }
   ],
   "source": [
    "# Save Enhanced RAG Results\n",
    "print(\"💾 Saving Enhanced RAG results...\")\n",
    "\n",
    "# Save to CSV\n",
    "queries.to_csv(\"../results/enhanced_rag_answers.csv\", index=False)\n",
    "print(\"Enhanced RAG results saved to: ../results/enhanced_rag_answers.csv\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\n📊 Sample Enhanced RAG Results:\")\n",
    "display_columns = ['question', 'answer', 'generated_answer', 'combined_context']\n",
    "sample_results = queries[display_columns].head(5)\n",
    "\n",
    "for idx, row in sample_results.iterrows():\n",
    "    print(f\"\\n--- Query {idx + 1} ---\")\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Ground Truth: {row['answer']}\")\n",
    "    print(f\"Enhanced RAG Answer: {row['generated_answer']}\")\n",
    "    print(f\"Context Length: {len(row['combined_context'])} characters\")\n",
    "    print(f\"Context Preview: {row['combined_context'][:150]}...\")\n",
    "\n",
    "print(f\"\\n✅ Enhanced RAG processing completed!\")\n",
    "print(f\"Results saved with query rewriting and reranking improvements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d1fd6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Removed temporary file: ../results/enhanced_rag_generated_answers_temp.csv\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temporary files\n",
    "import os\n",
    "\n",
    "# Remove temporary file if exists\n",
    "temp_file_path = \"../results/enhanced_rag_generated_answers_temp.csv\"\n",
    "if os.path.exists(temp_file_path):\n",
    "    os.remove(temp_file_path)\n",
    "    print(f\"🗑️ Removed temporary file: {temp_file_path}\")\n",
    "else:\n",
    "    print(\"No temporary file to clean up\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
